{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import statements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import statistics\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "import calendar\n",
    "import datetime\n",
    "import string\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and clean data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and clean reddit data\n",
    "reddit = pd.read_csv(\"reddit_clean_1.csv\")\n",
    "reddit = reddit.rename(columns = {\"title\": \"Headlines\"}).dropna(subset=[\"Headlines\"]).reset_index()\n",
    "reddit_50 = reddit.iloc[0:50,:]\n",
    "\n",
    "# Import and clean seeking alpha analyst headlines\n",
    "sa = pd.read_csv(\"SeekingAlpha_Coded.csv\")\n",
    "sa_100 = sa.iloc[0:100,:]\n",
    "\n",
    "# Import and clean seeking alpha news headlines\n",
    "san = pd.read_csv(\"SeekingAlpha_News_Coded.csv\")\n",
    "san_75 = san.iloc[0:75,:]\n",
    "\n",
    "# Import and clean Twitter data\n",
    "tw = pd.read_csv(\"Twitter_clean_1.csv\")\n",
    "tw = tw.rename(columns = {\"Text\": \"Headlines\"}).dropna(subset=[\"Headlines\"]).reset_index()\n",
    "tw_50 = tw.iloc[0:50,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get top 100 positive, negative and neutral words\n",
    "\n",
    "def top_100 (df):\n",
    "    \n",
    "    #train test split\n",
    "    X = df.iloc[:, 1:-1]\n",
    "    y = df.iloc[:,-1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "    \n",
    "    #tuning nltk \n",
    "    X_y_train = X_train.join(y_train).reset_index()\n",
    "    X_y_test = X_test.join(y_test).reset_index()\n",
    "    \n",
    "    pos = X_y_train[X_y_train[\"Classification\"] == \"pos\"][\"Headlines\"]\n",
    "    neg = X_y_train[X_y_train[\"Classification\"] == \"neg\"][\"Headlines\"]\n",
    "    neu = X_y_train[X_y_train[\"Classification\"] == \"neu\"][\"Headlines\"]\n",
    "    \n",
    "    pos_sentence_list = pos.apply(nltk.word_tokenize)\n",
    "    pos_word_list = [item.lower() for sublist in pos_sentence_list for item in sublist]\n",
    "    neg_sentence_list = neg.apply(nltk.word_tokenize)\n",
    "    neg_word_list = [item.lower() for sublist in neg_sentence_list for item in sublist]\n",
    "    neu_sentence_list = neu.apply(nltk.word_tokenize)\n",
    "    neu_word_list = [item.lower() for sublist in neu_sentence_list for item in sublist]\n",
    "\n",
    "    positive_fd = nltk.FreqDist(pos_word_list)\n",
    "    negative_fd = nltk.FreqDist(neg_word_list)\n",
    "    neutral_fd = nltk.FreqDist(neu_word_list)\n",
    "\n",
    "    common_set = set(neutral_fd).intersection(negative_fd).union(set(positive_fd).intersection(negative_fd)).union(set(positive_fd).intersection(neutral_fd))\n",
    "\n",
    "    for word in common_set:\n",
    "        del positive_fd[word]\n",
    "        del negative_fd[word]\n",
    "        del neutral_fd[word]\n",
    "\n",
    "    top_100_positive = [word for word, count in positive_fd.most_common(100)]\n",
    "    top_100_negative = [word for word, count in negative_fd.most_common(100)]\n",
    "    top_100_neutral = [word for word, count in neutral_fd.most_common(100)]\n",
    "    \n",
    "    return(top_100_positive, top_100_negative, top_100_neutral)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate bigram \n",
    "\n",
    "def generate_bigram(sentence):\n",
    "    \n",
    "    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = nltk.wordpunct_tokenize(sentence.lower())\n",
    "    finder = BigramCollocationFinder.from_words(tokens)\n",
    "    scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "    return(sorted(bigram for bigram, score in scored))\n",
    "\n",
    "def bigram_top_100(df):\n",
    "    \n",
    "    #train test split\n",
    "    X = df.iloc[:, 1:-1]\n",
    "    y = df.iloc[:,-1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "    \n",
    "    #tuning nltk \n",
    "    X_y_train = X_train.join(y_train).reset_index()\n",
    "    X_y_test = X_test.join(y_test).reset_index()\n",
    "        \n",
    "    pos = X_y_train[X_y_train[\"Classification\"] == \"pos\"][\"Headlines\"]\n",
    "    neg = X_y_train[X_y_train[\"Classification\"] == \"neg\"][\"Headlines\"]\n",
    "    neu = X_y_train[X_y_train[\"Classification\"] == \"neu\"][\"Headlines\"]\n",
    "        \n",
    "    pos_sentence_list_bigram = pos.apply(generate_bigram)\n",
    "    pos_word_list_bigram = [item for sublist in pos_sentence_list_bigram for item in sublist]\n",
    "    neg_sentence_list_bigram = neg.apply(generate_bigram)\n",
    "    neg_word_list_bigram = [item for sublist in neg_sentence_list_bigram for item in sublist]\n",
    "    neu_sentence_list_bigram = neu.apply(generate_bigram)\n",
    "    neu_word_list_bigram = [item for sublist in neu_sentence_list_bigram for item in sublist]\n",
    "\n",
    "    positive_fd_bigram = nltk.FreqDist(pos_word_list_bigram)\n",
    "    negative_fd_bigram = nltk.FreqDist(neg_word_list_bigram)\n",
    "    neutral_fd_bigram = nltk.FreqDist(neu_word_list_bigram)\n",
    "\n",
    "    common_set_bigram = set(neutral_fd_bigram).intersection(negative_fd_bigram).union(set(positive_fd_bigram).intersection(negative_fd_bigram)).union(set(positive_fd_bigram).intersection(neutral_fd_bigram))\n",
    "\n",
    "    for word in common_set_bigram:\n",
    "        del positive_fd_bigram[word]\n",
    "        del negative_fd_bigram[word]\n",
    "        del neutral_fd_bigram[word]\n",
    "\n",
    "    top_100_positive_bigram = {word for word, count in positive_fd_bigram.most_common(100)}\n",
    "    top_100_negative_bigram = {word for word, count in negative_fd_bigram.most_common(100)}\n",
    "    top_100_neutral_bigram = {word for word, count in neutral_fd_bigram.most_common(100)}\n",
    "    return(top_100_positive_bigram, top_100_negative_bigram, top_100_neutral_bigram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('2021', 'or'),\n",
       " ('30', '2020'),\n",
       " ('40k', 'on'),\n",
       " ('7', 'billion'),\n",
       " ('800c', 'exp'),\n",
       " ('a', 'cookie'),\n",
       " ('a', 'model'),\n",
       " ('a', 'picture'),\n",
       " ('actually', 'from'),\n",
       " ('and', 'more'),\n",
       " ('berlin', 'one'),\n",
       " ('billion', 'dollars'),\n",
       " ('bought', 'a'),\n",
       " ('build', 'progress'),\n",
       " ('cant', 'delete'),\n",
       " ('cookie', 'on'),\n",
       " ('cost', 'investors'),\n",
       " ('covered', '800c'),\n",
       " ('daily', 'investor'),\n",
       " ('dateprice', 'conjecture'),\n",
       " ('december', '30'),\n",
       " ('delete', 'this'),\n",
       " ('discussion', 'december'),\n",
       " ('do', 'we'),\n",
       " ('dopfner', 'tesla'),\n",
       " ('drip', 'feed'),\n",
       " ('ellison', 'in'),\n",
       " ('elon', 'actually'),\n",
       " ('elon', 'musk'),\n",
       " ('exp', '0108'),\n",
       " ('feed', 'over'),\n",
       " ('for', 'days'),\n",
       " ('for', 'q4'),\n",
       " ('from', 'plantlabs'),\n",
       " ('from', 'the'),\n",
       " ('front', 'page'),\n",
       " ('fsd', 'subscription'),\n",
       " ('funds', 'cost'),\n",
       " ('gang', 'wya'),\n",
       " ('get', 'the'),\n",
       " ('gigafactory', 'berlin'),\n",
       " ('had', 'existed'),\n",
       " ('hawaii', 'honolulu'),\n",
       " ('here', 'for'),\n",
       " ('honolulu', 'staradvertiser'),\n",
       " ('if', 'it'),\n",
       " ('in', '2021'),\n",
       " ('in', 'hawaii'),\n",
       " ('index', 'funds'),\n",
       " ('investor', 'discussion'),\n",
       " ('investors', '7'),\n",
       " ('is', 'elon'),\n",
       " ('is', 'what'),\n",
       " ('it', 'had'),\n",
       " ('larry', 'ellison'),\n",
       " ('left', 'a'),\n",
       " ('like', 'if'),\n",
       " ('linus', 'bought'),\n",
       " ('look', 'like'),\n",
       " ('margin', 'questions'),\n",
       " ('meets', 'with'),\n",
       " ('month', 'of'),\n",
       " ('musk', 'meets'),\n",
       " ('numbers', 'for'),\n",
       " ('of', 'a'),\n",
       " ('of', 'build'),\n",
       " ('on', 'tesla'),\n",
       " ('on', 'the'),\n",
       " ('one', 'month'),\n",
       " ('or', 'drip'),\n",
       " ('oracles', 'larry'),\n",
       " ('over', 'the'),\n",
       " ('page', 'here'),\n",
       " ('picture', 'of'),\n",
       " ('plantlabs', 'satellite'),\n",
       " ('powerbank', 'would'),\n",
       " ('progress', 'from'),\n",
       " ('q4', 'deliveries'),\n",
       " ('questions', 'specifically'),\n",
       " ('regarding', 'tsla'),\n",
       " ('selling', 'tsla'),\n",
       " ('shares', 'in'),\n",
       " ('specifically', 'regarding'),\n",
       " ('start', 'dateprice'),\n",
       " ('subscription', 'start'),\n",
       " ('tesla', 'powerbank'),\n",
       " ('tesla', 'shares'),\n",
       " ('the', 'front'),\n",
       " ('the', 'future'),\n",
       " ('the', 'numbers'),\n",
       " ('the', 'year'),\n",
       " ('this', 'we'),\n",
       " ('tsla', 'covered'),\n",
       " ('tsla', 'daily'),\n",
       " ('we', 'get'),\n",
       " ('we', 'left'),\n",
       " ('what', 'tesla'),\n",
       " ('when', 'do'),\n",
       " ('with', 'oracles'),\n",
       " ('would', 'look')}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_top_100(reddit_50)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to filter stop words \n",
    "# for word vect\n",
    "\n",
    "def remove_stop_words(func):\n",
    "    \n",
    "    stop_words=set(stopwords.words(\"english\"))\n",
    "\n",
    "    filtered_pos = []\n",
    "    filtered_neg = []\n",
    "    filtered_neu = []\n",
    "    \n",
    "    pos = func[0]\n",
    "    neg = func[1]\n",
    "    neu = func[2]\n",
    "    \n",
    "    for positive in pos:\n",
    "        if positive not in stop_words:\n",
    "            filtered_pos.append(positive)\n",
    "\n",
    "    for neutral in neu:\n",
    "        if neutral not in stop_words:\n",
    "            filtered_neu.append(neutral)\n",
    "\n",
    "    for negative in neg:\n",
    "        if negative not in stop_words:\n",
    "            filtered_neg.append(negative)\n",
    "            \n",
    "    return(filtered_pos, filtered_neg, filtered_neu)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reddit_50\n",
    "#top_100(reddit_50)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Dataset Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df, df1):\n",
    "    \n",
    "    top_100_positive = top_100(df1)[0]\n",
    "    top_100_negative = top_100(df1)[1]\n",
    "    top_100_neutral = top_100(df1)[2]\n",
    "    \n",
    "    top_100_positive_bigram = bigram_top_100(df1)[0]\n",
    "    top_100_negative_bigram = bigram_top_100(df1)[1]\n",
    "    top_100_neutral_bigram = bigram_top_100(df1)[2]\n",
    "    \n",
    "    features = dict()\n",
    "    combined_pos_count = list()\n",
    "    combined_neg_count = list()\n",
    "    combined_neu_count = list()\n",
    "    combined_pos_count_bigram = list()\n",
    "    combined_neg_count_bigram = list()\n",
    "    combined_neu_count_bigram = list()\n",
    "    combined_compound_scores = list()\n",
    "    combined_positive_scores = list()\n",
    "    combined_negative_scores = list()\n",
    "    combined_neutral_scores = list()\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        sentence = df[\"Headlines\"][i]\n",
    "        pos_count = 0\n",
    "        neg_count = 0\n",
    "        neu_count = 0\n",
    "        pos_bigram_count = 0\n",
    "        neg_bigram_count = 0\n",
    "        neu_bigram_count = 0\n",
    "        \n",
    "        for word in nltk.word_tokenize(sentence):\n",
    "            if word.lower() in top_100_positive:\n",
    "                pos_count += 1\n",
    "            if word.lower() in top_100_negative:\n",
    "                neg_count += 1\n",
    "            if word.lower() in top_100_neutral:\n",
    "                neu_count += 1\n",
    "                \n",
    "        for bigram in generate_bigram(sentence):\n",
    "            if bigram in top_100_positive_bigram:\n",
    "                pos_bigram_count += 1\n",
    "            if bigram in top_100_negative_bigram:\n",
    "                neg_bigram_count += 1\n",
    "            if bigram in top_100_neutral_bigram:\n",
    "                neu_bigram_count += 1\n",
    "\n",
    "        combined_compound_scores.append(sia.polarity_scores(sentence)[\"compound\"] + 1)\n",
    "        combined_positive_scores.append(sia.polarity_scores(sentence)[\"pos\"])\n",
    "        combined_negative_scores.append(sia.polarity_scores(sentence)[\"neg\"])\n",
    "        combined_neutral_scores.append(sia.polarity_scores(sentence)[\"neu\"])\n",
    "        combined_pos_count.append(pos_count)\n",
    "        combined_neg_count.append(neg_count)\n",
    "        combined_neu_count.append(neu_count)\n",
    "        combined_pos_count_bigram.append(pos_bigram_count)\n",
    "        combined_neg_count_bigram.append(neg_bigram_count)\n",
    "        combined_neu_count_bigram.append(neu_bigram_count)\n",
    "    \n",
    "    features[\"headlines\"] = df[\"Headlines\"]\n",
    "    features[\"compound\"] = combined_compound_scores\n",
    "    features[\"positive\"] = combined_positive_scores\n",
    "    features[\"negative\"] = combined_negative_scores\n",
    "    features[\"neutral\"] = combined_neutral_scores\n",
    "    features[\"pos_count\"] = combined_pos_count\n",
    "    features[\"neg_count\"] = combined_neg_count\n",
    "    features[\"neu_count\"] = combined_neu_count\n",
    "    features[\"pos_count_bigram\"] = combined_pos_count_bigram\n",
    "    features[\"neg_count_bigram\"] = combined_neg_count_bigram\n",
    "    features[\"neu_count_bigram\"] = combined_neu_count_bigram\n",
    "    #features[\"Classification\"] = input_data[\"Classification\"]\n",
    "    \n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>compound</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>pos_count</th>\n",
       "      <th>neg_count</th>\n",
       "      <th>neu_count</th>\n",
       "      <th>pos_count_bigram</th>\n",
       "      <th>neg_count_bigram</th>\n",
       "      <th>neu_count_bigram</th>\n",
       "      <th>Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>papa Elon spoke and we listened, the prophecy ...</td>\n",
       "      <td>1.4215</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.781</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tesla's exponential growth visualized. As Elon...</td>\n",
       "      <td>1.7003</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.756</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tesla could reach 500,000 deliveries in 2020 t...</td>\n",
       "      <td>1.7351</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.690</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FSD Subscription Start Date/Price Conjecture</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Captured a glorious moment today</td>\n",
       "      <td>1.6369</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.417</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>'Big Short' investor Michael Burry has already...</td>\n",
       "      <td>0.3631</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.785</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>69420</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Margin questions, specifically regarding TSLA</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Elon Musk meets with Oracles Larry Ellison in ...</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>$TSLA Daily Investor Discussion - December 30,...</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Gang WYA</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>40K on tesla shares in 2021 or drip feed over ...</td>\n",
       "      <td>1.2960</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Linus bought a Model Y!</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>When do we get the numbers for Q4 deliveries?</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Gigafactory Berlin one month of build progress...</td>\n",
       "      <td>1.4215</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.763</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Tesla 2020: Jim Cramer, Tesla Daily's Rob Maur...</td>\n",
       "      <td>0.4426</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.753</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Tesla Autopilot FSD San Francisco to Los Angel...</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Index Funds cost investors 7 billion dollars</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Tesla Model S and Model X refresh practically ...</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>UPS reserves 125 Tesla semi-trucks, largest pu...</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>This is what Tesla PowerBank would look like i...</td>\n",
       "      <td>1.3612</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2021 Model 3 Delivery, First Drive, and New Fe...</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Cant delete this we left a picture of a cookie...</td>\n",
       "      <td>1.2263</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Selling TSLA Covered 800c exp 01/08</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Selling TSLA Covered 800C exp 01/08</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Tesla reaches five-year lithium deal with Chin...</td>\n",
       "      <td>1.0516</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.854</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Is Elon actually from the future?</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Elons interview with Dopfner: Tesla long term ...</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>We just received about 300 documents about $TS...</td>\n",
       "      <td>1.4995</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>$TSLA Daily Investor Discussion - December 31,...</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Selling TSLA Covered 800c exp 12/08</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Tesla Short Sellers Lost $38 Billion in 2020 a...</td>\n",
       "      <td>0.6818</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.813</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Model 3 #1 Model Y #3 in Global Deliveries Nov...</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Tesla stock (TSLA) pushes to new all-time high...</td>\n",
       "      <td>0.6818</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.892</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Dont bet against elon</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Tesla Model X spotted equipped with LiDAR sens...</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>It has been done! Congrats to to my fellow TIC...</td>\n",
       "      <td>1.5707</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.802</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headlines  compound  positive  \\\n",
       "0   papa Elon spoke and we listened, the prophecy ...    1.4215     0.219   \n",
       "1   Tesla's exponential growth visualized. As Elon...    1.7003     0.244   \n",
       "2   Tesla could reach 500,000 deliveries in 2020 t...    1.7351     0.310   \n",
       "3        FSD Subscription Start Date/Price Conjecture    1.0000     0.000   \n",
       "4                    Captured a glorious moment today    1.6369     0.583   \n",
       "5   'Big Short' investor Michael Burry has already...    0.3631     0.000   \n",
       "6                                               69420    1.0000     0.000   \n",
       "7       Margin questions, specifically regarding TSLA    1.0000     0.000   \n",
       "8   Elon Musk meets with Oracles Larry Ellison in ...    1.0000     0.000   \n",
       "9   $TSLA Daily Investor Discussion - December 30,...    1.0000     0.000   \n",
       "10                                           Gang WYA    1.0000     0.000   \n",
       "11  40K on tesla shares in 2021 or drip feed over ...    1.2960     0.167   \n",
       "12                            Linus bought a Model Y!    1.0000     0.000   \n",
       "13      When do we get the numbers for Q4 deliveries?    1.0000     0.000   \n",
       "14  Gigafactory Berlin one month of build progress...    1.4215     0.237   \n",
       "15  Tesla 2020: Jim Cramer, Tesla Daily's Rob Maur...    0.4426     0.000   \n",
       "16  Tesla Autopilot FSD San Francisco to Los Angel...    1.0000     0.000   \n",
       "17       Index Funds cost investors 7 billion dollars    1.0000     0.000   \n",
       "18  Tesla Model S and Model X refresh practically ...    1.0000     0.000   \n",
       "19  UPS reserves 125 Tesla semi-trucks, largest pu...    1.0000     0.000   \n",
       "20  This is what Tesla PowerBank would look like i...    1.3612     0.185   \n",
       "21  2021 Model 3 Delivery, First Drive, and New Fe...    1.0000     0.000   \n",
       "22  Cant delete this we left a picture of a cookie...    1.2263     0.112   \n",
       "23                Selling TSLA Covered 800c exp 01/08    1.0000     0.000   \n",
       "24                Selling TSLA Covered 800C exp 01/08    1.0000     0.000   \n",
       "25  Tesla reaches five-year lithium deal with Chin...    1.0516     0.146   \n",
       "26                  Is Elon actually from the future?    1.0000     0.000   \n",
       "27  Elons interview with Dopfner: Tesla long term ...    1.0000     0.000   \n",
       "28  We just received about 300 documents about $TS...    1.4995     0.071   \n",
       "29  $TSLA Daily Investor Discussion - December 31,...    1.0000     0.000   \n",
       "30                Selling TSLA Covered 800c exp 12/08    1.0000     0.000   \n",
       "31  Tesla Short Sellers Lost $38 Billion in 2020 a...    0.6818     0.000   \n",
       "32  Model 3 #1 Model Y #3 in Global Deliveries Nov...    1.0000     0.000   \n",
       "33  Tesla stock (TSLA) pushes to new all-time high...    0.6818     0.000   \n",
       "34                              Dont bet against elon    1.0000     0.000   \n",
       "35  Tesla Model X spotted equipped with LiDAR sens...    1.0000     0.000   \n",
       "36  It has been done! Congrats to to my fellow TIC...    1.5707     0.198   \n",
       "\n",
       "    negative  neutral  pos_count  neg_count  neu_count  pos_count_bigram  \\\n",
       "0      0.000    0.781          7          0          0                10   \n",
       "1      0.000    0.756         16          0          0                20   \n",
       "2      0.000    0.690         12          0          0                18   \n",
       "3      0.000    1.000          0          0          4                 0   \n",
       "4      0.000    0.417          4          0          0                 4   \n",
       "5      0.215    0.785         19          0          0                20   \n",
       "6      0.000    1.000          0          0          1                 0   \n",
       "7      0.000    1.000          0          0          4                 0   \n",
       "8      0.000    1.000          0          0          9                 0   \n",
       "9      0.000    1.000          0          0          3                 0   \n",
       "10     0.000    1.000          0          0          2                 0   \n",
       "11     0.000    0.833          0          0          8                 0   \n",
       "12     0.000    1.000          0          0          2                 0   \n",
       "13     0.000    1.000          0          0          7                 0   \n",
       "14     0.000    0.763          0          0          9                 0   \n",
       "15     0.247    0.753          8          0          0                11   \n",
       "16     0.000    1.000          7          0          0                10   \n",
       "17     0.000    1.000          0          0          6                 0   \n",
       "18     0.000    1.000          8          0          0                 2   \n",
       "19     0.000    1.000          8          0          0                 0   \n",
       "20     0.000    0.815          0          0          8                 0   \n",
       "21     0.000    1.000          7          0          0                 1   \n",
       "22     0.000    0.888          0          0         11                 0   \n",
       "23     0.000    1.000          0          0          5                 0   \n",
       "24     0.000    1.000          0          0          5                 0   \n",
       "25     0.000    0.854          3          0          0                 0   \n",
       "26     0.000    1.000          0          0          4                 0   \n",
       "27     0.000    1.000          0          0          7                 0   \n",
       "28     0.000    0.929          0          0         23                 0   \n",
       "29     0.000    1.000          0          0          2                 0   \n",
       "30     0.000    1.000          0          0          4                 0   \n",
       "31     0.187    0.813          5          0          0                 4   \n",
       "32     0.000    1.000          4          0          0                 1   \n",
       "33     0.108    0.892          8          0          0                 5   \n",
       "34     0.000    1.000          2          0          0                 1   \n",
       "35     0.000    1.000          1          0          0                 2   \n",
       "36     0.000    0.802          2          0          0                 1   \n",
       "\n",
       "    neg_count_bigram  neu_count_bigram Classification  \n",
       "0                  0                 0            pos  \n",
       "1                  0                 0            pos  \n",
       "2                  0                 0            pos  \n",
       "3                  0                 4            neu  \n",
       "4                  0                 0            pos  \n",
       "5                  0                 0            pos  \n",
       "6                  0                 0            neu  \n",
       "7                  0                 4            neu  \n",
       "8                  0                10            neu  \n",
       "9                  0                 6            neu  \n",
       "10                 0                 1            neu  \n",
       "11                 0                11            neu  \n",
       "12                 0                 3            neu  \n",
       "13                 0                 8            neu  \n",
       "14                 0                 9            neu  \n",
       "15                 0                 0            pos  \n",
       "16                 0                 0            pos  \n",
       "17                 0                 6            neu  \n",
       "18                 0                 0            pos  \n",
       "19                 0                 0            pos  \n",
       "20                 0                10            neu  \n",
       "21                 0                 0            pos  \n",
       "22                 0                16            neu  \n",
       "23                 0                 5            neu  \n",
       "24                 0                 5            neu  \n",
       "25                 0                 0            pos  \n",
       "26                 0                 5            neu  \n",
       "27                 0                 2            neu  \n",
       "28                 0                 1            neu  \n",
       "29                 0                 4            neu  \n",
       "30                 0                 4            neu  \n",
       "31                 0                 0            pos  \n",
       "32                 0                 0            pos  \n",
       "33                 0                 0            pos  \n",
       "34                 0                 0            pos  \n",
       "35                 0                 0            pos  \n",
       "36                 0                 0            pos  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = reddit_50.iloc[:, 1:-1]\n",
    "y = reddit_50.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "X_y_train = X_train.join(y_train).reset_index()\n",
    "X_y_test = X_test.join(y_test).reset_index()\n",
    "\n",
    "\n",
    "X_y_train_extract = pd.DataFrame(extract_features(X_y_train, reddit_50))\n",
    "X_y_train_extract[\"Classification\"] = X_y_train[\"Classification\"]\n",
    "X_y_test_extract = pd.DataFrame(extract_features(X_y_test, reddit_50))\n",
    "X_y_test_extract[\"Classification\"] = X_y_test[\"Classification\"]\n",
    "\n",
    "X_y_train_extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial NB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_nb (df):\n",
    "\n",
    "    X = df.iloc[:, 1:-1]\n",
    "    y = df.iloc[:,-1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "    X_y_train = X_train.join(y_train).reset_index()\n",
    "    X_y_test = X_test.join(y_test).reset_index()\n",
    "\n",
    "    #X_y_train_extract = pd.DataFrame(extract_features(X_y_train))\n",
    "    #X_y_test_extract = pd.DataFrame(extract_features(X_y_test))\n",
    "    \n",
    "    # choose bigram or tokenization\n",
    "    \n",
    "    X_y_train_extract = pd.DataFrame(extract_features(X_y_train, df)).drop([\"pos_count\", \"neg_count\", \"neu_count\"], axis = 1)\n",
    "    X_y_train_extract[\"Classification\"] = X_y_train[\"Classification\"]\n",
    "    X_y_test_extract = pd.DataFrame(extract_features(X_y_test, df)).drop([\"pos_count\", \"neg_count\", \"neu_count\"], axis = 1)\n",
    "    X_y_test_extract[\"Classification\"] = X_y_test[\"Classification\"]\n",
    "\n",
    "    X_train = X_y_train_extract.iloc[:, 1:-1]\n",
    "    y_train = X_y_train_extract.iloc[:, -1]\n",
    "    X_test = X_y_test_extract.iloc[:, 1:-1]\n",
    "    y_test = X_y_test_extract.iloc[:, -1]\n",
    "\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train, y_train)\n",
    "    y_pred_prob = nb.predict(X_test)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for i in range(len(y_pred_prob)):\n",
    "        if y_pred_prob[i] == np.array(y_test)[i]:\n",
    "            count += 1\n",
    "\n",
    "    bi_accuracy = count/len(y_pred_prob)\n",
    "\n",
    "    # =============================================================\n",
    "    \n",
    "    X = df.iloc[:, 1:-1]\n",
    "    y = df.iloc[:,-1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "    X_y_train = X_train.join(y_train).reset_index()\n",
    "    X_y_test = X_test.join(y_test).reset_index()\n",
    "    \n",
    "    X_y_train_extract = pd.DataFrame(extract_features(X_y_train, df)).drop([\"pos_count_bigram\", \"neg_count_bigram\", \"neu_count_bigram\"], axis = 1)\n",
    "    X_y_train_extract[\"Classification\"] = X_y_train[\"Classification\"]\n",
    "    X_y_test_extract = pd.DataFrame(extract_features(X_y_test, df)).drop([\"pos_count_bigram\", \"neg_count_bigram\", \"neu_count_bigram\"], axis = 1)\n",
    "    X_y_test_extract[\"Classification\"] = X_y_test[\"Classification\"]\n",
    "    \n",
    "    X_train = X_y_train_extract.iloc[:, 1:-1]\n",
    "    y_train = X_y_train_extract.iloc[:, -1]\n",
    "    X_test = X_y_test_extract.iloc[:, 1:-1]\n",
    "    y_test = X_y_test_extract.iloc[:, -1]\n",
    "\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train, y_train)\n",
    "    y_pred_prob = nb.predict(X_test)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for i in range(len(y_pred_prob)):\n",
    "        if y_pred_prob[i] == np.array(y_test)[i]:\n",
    "            count += 1\n",
    "\n",
    "    vect_accuracy = count/len(y_pred_prob)\n",
    "    \n",
    "    # =============================================================\n",
    "    \n",
    "    X = df.iloc[:, 1:-1]\n",
    "    y = df.iloc[:,-1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "    X_y_train = X_train.join(y_train).reset_index()\n",
    "    X_y_test = X_test.join(y_test).reset_index()\n",
    "    \n",
    "    X_y_train_extract = pd.DataFrame(extract_features(X_y_train, df))\n",
    "    X_y_train_extract[\"Classification\"] = X_y_train[\"Classification\"]\n",
    "    X_y_test_extract = pd.DataFrame(extract_features(X_y_test, df))\n",
    "    X_y_test_extract[\"Classification\"] = X_y_test[\"Classification\"]\n",
    "    \n",
    "    X_train = X_y_train_extract.iloc[:, 1:-1]\n",
    "    y_train = X_y_train_extract.iloc[:, -1]\n",
    "    X_test = X_y_test_extract.iloc[:, 1:-1]\n",
    "    y_test = X_y_test_extract.iloc[:, -1]\n",
    "\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train, y_train)\n",
    "    y_pred_prob = nb.predict(X_test)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for i in range(len(y_pred_prob)):\n",
    "        if y_pred_prob[i] == np.array(y_test)[i]:\n",
    "            count += 1\n",
    "\n",
    "    combine_accuracy = count/len(y_pred_prob)\n",
    "    \n",
    "    if (bi_accuracy > vect_accuracy) & (bi_accuracy > combine_accuracy): \n",
    "        print (\"Bigram performs the best with accuracy of:\", bi_accuracy)\n",
    "    elif (combine_accuracy > bi_accuracy) & (combine_accuracy > vect_accuracy):\n",
    "        print(\"Combined performs the best with accuracy of:\", combine_accuracy)\n",
    "    else: \n",
    "        print(\"Vectorization performs the best with accuracy of:\", vect_accuracy)\n",
    "    print(\"\\n\")    \n",
    "    print(\"Bigram accuracy: \", bi_accuracy)\n",
    "    print(\"Combined accuracy: \", combine_accuracy)\n",
    "    print(\"Vectorization accuracy: \", vect_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram performs the best with accuracy of: 0.7692307692307693\n",
      "\n",
      "\n",
      "Bigram accuracy:  0.7692307692307693\n",
      "Combined accuracy:  0.6923076923076923\n",
      "Vectorization accuracy:  0.6153846153846154\n"
     ]
    }
   ],
   "source": [
    "accuracy_nb(reddit_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization performs the best with accuracy of: 0.6923076923076923\n",
      "\n",
      "\n",
      "Bigram accuracy:  0.5384615384615384\n",
      "Combined accuracy:  0.6923076923076923\n",
      "Vectorization accuracy:  0.6923076923076923\n"
     ]
    }
   ],
   "source": [
    "accuracy_nb(tw_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization performs the best with accuracy of: 0.631578947368421\n",
      "\n",
      "\n",
      "Bigram accuracy:  0.631578947368421\n",
      "Combined accuracy:  0.5789473684210527\n",
      "Vectorization accuracy:  0.631578947368421\n"
     ]
    }
   ],
   "source": [
    "accuracy_nb(san_75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization performs the best with accuracy of: 0.64\n",
      "\n",
      "\n",
      "Bigram accuracy:  0.52\n",
      "Combined accuracy:  0.6\n",
      "Vectorization accuracy:  0.64\n"
     ]
    }
   ],
   "source": [
    "accuracy_nb(sa_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
